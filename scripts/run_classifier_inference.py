#!/usr/bin/env python3
"""Run classifier inference on a manifest of crops and write predictions CSV."""

from __future__ import annotations

import argparse
import csv
from pathlib import Path
from typing import Dict, List

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from PIL import Image

try:
    import yaml  # type: ignore
except ModuleNotFoundError as exc:  # pragma: no cover
    raise RuntimeError("PyYAML is required to run classifier inference") from exc

from training import data_utils
from training import train_classifier as training_train_classifier


class InferenceDataset(Dataset):
    """Dataset that returns crop image tensors plus raw metadata."""

    def __init__(self, manifest_rows: List[Dict[str, str]], data_root: Path, transform, normalise: bool = True) -> None:
        self.records: List[Dict[str, str]] = []
        for row in manifest_rows:
            rel_path = data_utils.normalize_rel_path(row["crop_path"])
            full_path = data_root / Path(rel_path)
            if not full_path.exists():
                continue
            row = dict(row)
            row["crop_path_norm"] = rel_path
            row["crop_path_full"] = str(full_path)
            self.records.append(row)
        if not self.records:
            raise ValueError("No crops found for inference. Check manifest paths.")
        self.transform = transform

    def __len__(self) -> int:
        return len(self.records)

    def __getitem__(self, idx: int):
        record = self.records[idx]
        image = Image.open(record["crop_path_full"]).convert("RGB")
        image = self.transform(image)
        return image, record


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run trained classifier on auto-labeled crops")
    parser.add_argument("--checkpoint", required=True, help="Path to trained model checkpoint (best_model.pt)")
    parser.add_argument("--manifest", default="data/crops_manifest.csv", help="Crops manifest generated by autolabel step")
    parser.add_argument("--output", default="experiments/exp_003_species/predictions_inference.csv", help="Where to write per-crop predictions")
    parser.add_argument("--batch-size", type=int, default=128, help="Batch size for inference")
    parser.add_argument("--device", default="cuda", help="Device to use (cuda/cpu/auto)")
    parser.add_argument("--image-size", type=int, help="Override image size (otherwise read from checkpoint)")
    return parser.parse_args()


def auto_device(value: str) -> str:
    if value == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return value


def load_checkpoint(path: Path, device: str) -> Dict[str, object]:
    if not path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {path}")
    return torch.load(path, map_location=device)


def main() -> int:
    args = parse_args()
    device = auto_device(args.device)

    manifest_path = Path(args.manifest)
    manifest_rows = data_utils.load_manifest_rows(manifest_path)

    checkpoint = load_checkpoint(Path(args.checkpoint), device)
    meta = checkpoint.get("meta", {})
    cfg_meta = meta.get("config", {})
    species_to_idx = meta.get("species_to_idx")
    if not species_to_idx:
        raise ValueError("Checkpoint does not contain species_to_idx mapping")
    idx_to_species = {idx: species for species, idx in species_to_idx.items()}

    image_size = args.image_size or cfg_meta.get("image_size", 224)
    _, eval_tf = data_utils.build_transforms(image_size)

    dataset = InferenceDataset(manifest_rows, data_root=manifest_path.parent, transform=eval_tf)
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=cfg_meta.get("dataloader_workers", 4),
        pin_memory=True,
    )

    model = training_train_classifier.build_model(cfg_meta.get("model_name", "resnet50"), len(species_to_idx), device)
    model.load_state_dict(checkpoint["model_state"])
    model.eval()

    predictions: List[Dict[str, object]] = []

    with torch.no_grad():
        for images, records in dataloader:
            images = images.to(device)
            outputs = model(images)
            probs = F.softmax(outputs, dim=1)
            confs, preds = torch.max(probs, dim=1)
            batch_size = images.size(0)
            for i in range(batch_size):
                record = {key: records[key][i] for key in records}
                predictions.append(
                    {
                        "crop_path": record.get("crop_path_norm", record.get("crop_path", "")),
                        "species_pred": idx_to_species[preds[i].item()],
                        "confidence": confs[i].item(),
                        "video": record.get("video", ""),
                        "video_stem": record.get("video_stem", ""),
                        "track_id": record.get("track_id", ""),
                        "frame_index": record.get("frame_index", ""),
                    }
                )

    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=predictions[0].keys())
        writer.writeheader()
        writer.writerows(predictions)

    print(f"Saved predictions to {output_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
